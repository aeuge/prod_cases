### Q:
Есть идея нанять препода, чтобы нам рассказал как юзать АИ в закрытом контуре для наших данных(бесплатно для членов сообщества). Как вам?

### A1:
llama?

### A2: 
Ollama ооочень медленная, если нет gpu, но и с gpu скорее всего проиграет. Cильно быстрее lm-studio, если локальная рабочая машина, и я не проверял, но наблюдал, vllm как сервер.
На core i9 64g nogpu lm-studio выводит ответ в темпе принтера строчка за строчкой. Не моментально, но комфортно для чтения. Примерно так, как глазами пробегаешь. Тоже самое ollama делает в темпе по слову в полсекунды или хуже даже. Создается впечатление, что общаешься с контуженным(конечно на прошке, где gpu есть, там все сильно лучше), но естественно, работая с llm хочется не столько получать 100% готовые ответы, а эффективное автодополнение. И тут скрость - ключевое качество.
У меня прошка с 16Г, потому я на nuc все поставил. И обломался (жду корпус и буду прикручивать внешнее gpu.) А так конечно надо замутить какой-то RAG по заэмбедденным исходникам корпоративных проектов. Пока изучаю)) 

### Q: 
Есть рабочие инструкции как развернуть?

### A2: 
lm-studio или ollama?
Линукс или макос?

### Q: 
Линукс или винда.

### A2: 
Винда увы. Хотя когда gpu прикручу не факт, что не попробую. А под линуксом все просто. Сейчас сваяю пару строк.

LM-Studio

Загрузить для выбранной платформы
https://lmstudio.ai/
https://dimensionquest.net/2024/09/how-i-install-lm-studio-0.3.2-on-ubuntu-studio-24.04-linux/
Схема запуска ./LM-Studio-0.3.14-5-x64.AppImage –no-sandbox
Типовое размещение
❯ sudo mkdir -p /opt/LMStudio
❯ sudo cp LM-Studio-0.3.14-5-x64.AppImage /opt/LMStudio
Иконка приложения на десктопе (lmstudio.png подобрать по вкусу)
❯ cat > ~/.local/share/applications/lmstudio.desktop << "EOF"
[Desktop Entry]
Name=LM Studio
Comсdio Launcher
Exec=/opt/LMStudio/LM_Studio.AppImage --no-sandbox
Icon=/usr/share/icons/lmstudio.png
Terminal=false
Type=Application
Categories=Development;
EOF

Ollama
https://ollama.com/download
Загрузка и установка приложения
❯ curl -fsSL https://ollama.com/install.sh | sh
Поскольку модели будут грузится в домашнюю папку сервиса, разумно переместить её
в тот раздел, который имеет подхлдящую емкость
❯ getent passwd ollama
ollama:x:993:993::/home/ollama:/bin/false
И сервис перезапустить
❯ systemctl status ollama
● ollama.service - Ollama Service
     Loaded: loaded (/etc/systemd/system/ollama.service; enabled; preset: enabled)
     Active: active (running) since Sun 2025-04-06 23:33:46 MSK; 6min ago
   Main PID: 8735 (ollama)
      Tasks: 12 (limit: 76908)
     Memory: 9.6M (peak: 11.4M)
        CPU: 46ms
     CGroup: /system.slice/ollama.service
             └─8735 /usr/local/bin/ollama serve
Выбираем подходящую модель
https://ollama.com/search
Например
https://ollama.com/library/deepseek-coder-v2:236b
133GB
И запускаем терминальный диалог
ollama run deepseek-coder-v2:236b

### Q:
Это рабочие инструкции ? Я не открывал.

### A2: 
Это цитаты с комментами из моего протокола установки. Я ставил на убунтовую станцию. Mate.
Оллама сразу сервер. Написана на голэнг. А лм-студио это десктопное приложение. Оно тоже работает как сервер, отдает openai/v1.
Лм-студио приятная штука. Там чат с моделью, есть уже предзагруженная ллм для эмбеддинга. И если её запустить как сервер (в гуе кнопка) то можно прямо на неё настроить vscode - если есть не дохлая прошка это самое оно!
Лм-студио приятная штука. Там чат с моделью, есть уже предзагруженная ллм для эмбеддинга. И если её запустить как сервер (в гуе кнопка) то можно прямо на неё настроить vscode - если есть не дохлая прошка это самое оно!
Вот как прикрутить vscode знаю, но прямо сейчас не скажу. Тут как раз надо проверять, чтобы была рабочая схема.))

### Q2:
А туда же можно и другие модельки загрузить к примеру от Алибабы?

### A2: 
Да. Все что qwen это как раз алибабашные.

### Q2:
На wsl не стоит ставить?

### A2: 
Там есть свой каталог. Он скромнее чем у ollama, но не сильно. И плюс лм-студия сразу пишет, что потянет твоя машина. Я вот в олламу явно вчухал что-то немерянное, может потому она тормозит(.
Там все не айс с прокладками типа докер и виртуалки. Приложение так или иначе должно получать доступ к gpu. И вот толи про мак (там докер виртуалка) толи про линукс было замечание, что из докера нет доступа к gpu. Т.е лучше на чистой платформе работать.
Я бы попробовал vllm. На гитхабе проект. Ожидаю, будет хорошо. И там каталог публичный ооочень большой!
